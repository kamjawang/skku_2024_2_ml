{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I used python kernel 3.10.15 + nvidia RTX 3090 + cuda 12.1 in local machine\n",
    "\n",
    "# install ralated modules\n",
    "%pip install torch==2.1.1 torchvision==0.16.1   # compatible version of pytorch and torchvision for mamba-ssm \n",
    "%pip install causal-conv1d==1.1.1   # causal dpthwise conv 1d  module in CUDA with pytorch\n",
    "%pip install mamba-ssm  # Mamba block module\n",
    "%pip install transtab # python package for transformer of tabluar data embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler,MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tqdm\n",
    "\n",
    "import transtab\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  2.1 ; cuda:  cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check environments\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get original data\n",
    "!mkdir ./datasets\n",
    "!mkdir ./datasets/adult\n",
    "!wget -nc https://archive.ics.uci.edu/static/public/2/adult.zip\n",
    "!unzip -o ./adult.zip -d ./datasets/adult\n",
    "!cp -rf ./datasets/adult/adult.data ./datasets/adult/data_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Configuration for MambaTab\n",
    "config={\n",
    "    'DATASET_NAME':'adult',\n",
    "    'SEED':15, # random seed 지정\n",
    "    'BATCH':100,\n",
    "    'LR':0.0001,\n",
    "    'EPOCH':100,\n",
    "    'MAMBA_SSM_DIM':32,  # MAMBA model의 dimension 설정 (d_model: Selective State Machine에 담을 최대 Dimension)\n",
    "    'device':'cuda'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오리지널 데이터 사용\n",
    "\n",
    "# data load and preparing\n",
    "def read_data(dataset_name):\n",
    "    data=pd.read_csv('./datasets/'+dataset_name+'/data_processed'+'.csv')\n",
    "    \n",
    "    #fill nulll values\n",
    "    for col in data.columns: \n",
    "        #data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "        data[col] = data[col].fillna(data[col].mode()[0])\n",
    "\n",
    "    #categorical encoder\n",
    "    for c in data.columns:\n",
    "        if is_string_dtype(data[c]):\n",
    "            data[c]=data[c].str.lower()\n",
    "            enc=OrdinalEncoder() # 레이블링 처리\n",
    "            cur_data=np.array(data[c])\n",
    "            cur_data=np.reshape(cur_data,(cur_data.shape[0],1))\n",
    "            data[c] = enc.fit_transform(cur_data)\n",
    "\n",
    "    y_data=data[data.columns[-1]]\n",
    "    x_data = data.drop(labels = [data.columns[-1]],axis = 1)\n",
    "    x_data=MinMaxScaler().fit_transform(x_data) # 컬럼 스케일링 처리\n",
    "    x_data,y_data=np.array(x_data),np.array(y_data)\n",
    "    return x_data,y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 같은 조 성준님 클렌징 데이터 사용\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "     # scikit-learn의 OpenML에서 성인 인구조사 소득 데이터셋 불러오기\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    data = fetch_openml(name='adult', version=1, as_frame=True)\n",
    "    df = data.frame\n",
    "\n",
    "    # 특성(features)과 목표(target) 변수 분리\n",
    "    # 'class' 열을 제외한 모든 열을 특성으로, 'class' 열을 목표 변수로 설정 >> X하고 y\n",
    "    X = df.drop('class', axis=1)\n",
    "    y = df['class']\n",
    "\n",
    "    # 열의 데이터 유형 식별\n",
    "    # 범주형(문자열) 열과 수치형(정수, 실수) 열 구분\n",
    "    categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "    numerical_columns = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    # 데이터 전처리 단계 생성\n",
    "    # ColumnTransformer를 사용하여 수치형과 범주형 열에 대해 다른 전처리 적용\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', Pipeline([                                                           # 수치형 열 처리\n",
    "                ('imputer', SimpleImputer(strategy='median')),                           # 결측값을 중앙값으로 대체\n",
    "                ('scaler', StandardScaler())                                             # 표준 스케일링 (평균 0, 분산 1로 정규화)\n",
    "            ]), numerical_columns),\n",
    "\n",
    "            ('cat', Pipeline([                                                           # 범주형 열 처리\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),   # 결측값을 'missing' 문자열로 대체\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore'))                       # 원핫 인코딩 (범주형 변수를 이진 벡터로 변환)\n",
    "            ]), categorical_columns)\n",
    "        ])\n",
    "\n",
    "    # 특성 데이터 전처리 수행\n",
    "    # 앞서 정의한 전처리기를 사용하여 데이터 변환\n",
    "    X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "    # 목표 변수 인코딩\n",
    "    # LabelEncoder를 사용하여 문자열 레이블을 숫자로 변환\n",
    "    le = LabelEncoder()\n",
    "    y_processed = le.fit_transform(y)\n",
    "\n",
    "    # 전처리된 특성, 목표 변수, 그리고 전처리기 반환\n",
    "    return X_processed, y_processed, preprocessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MambaTab Class\n",
    "\n",
    "class MambaTab(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,input_features,n_class,intermediate_representation=config['MAMBA_SSM_DIM']):\n",
    "        super(MambaTab, self).__init__()\n",
    "        self.linear_layer=torch.nn.Linear(input_features,intermediate_representation)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.layer_norm=torch.nn.LayerNorm(intermediate_representation)\n",
    "\n",
    "        self.mamba=Mamba(d_model=intermediate_representation, d_state=32, d_conv=4, expand=2) # to fine-tuning\n",
    "        self.output_layer=torch.nn.Linear(intermediate_representation,n_class)\n",
    "    \n",
    "    def forward(self, x):\n",
    "         x=self.linear_layer(x)\n",
    "         x=self.layer_norm(x)\n",
    "         x=self.relu(x)\n",
    "         x=self.mamba(x)\n",
    "         x=self.output_layer(x)\n",
    "         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def train_model(model,config, dataloader):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "    early_stopping_counter=0\n",
    "\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=config['LR'])  # Optimizer setting: Adam\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['EPOCH'], eta_min=0,verbose=False)\n",
    "    loss_fn=torch.nn.BCEWithLogitsLoss()    # 이진분류 처리를 위해 BCEWithLogitLoss 함수 사용\n",
    "  \n",
    "    for epoch in tqdm.tqdm(range(config['EPOCH'])):\n",
    "        if early_stopping_counter>=5:\n",
    "          break\n",
    "        \n",
    "        for phase in ['train', 'val']:      \n",
    "            if phase == 'train':               \n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()  \n",
    "            \n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "          \n",
    "            for btch,feed_dict in enumerate(dataloader[phase]):\n",
    "                inputs=feed_dict[0]\n",
    "                inputs=inputs.unsqueeze(0)\n",
    "                labels=feed_dict[1]\n",
    "                \n",
    "                inputs = inputs.type(torch.FloatTensor)\n",
    "                inputs = inputs.to(config['device'])\n",
    "                labels = labels.type(torch.FloatTensor)\n",
    "                labels = labels.to(config['device'])\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)  \n",
    "                    outputs=outputs.squeeze()  \n",
    "                    loss=loss_fn(outputs,labels)\n",
    "                    metrics['loss']+=loss.item()\n",
    "                \n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()  \n",
    "                \n",
    "                epoch_samples += 1 \n",
    "           \n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "\n",
    "            if phase == 'val':\n",
    "           \n",
    "                if epoch_loss<best_loss:\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    best_loss=epoch_loss\n",
    "                    early_stopping_counter=0\n",
    "                else:\n",
    "                    early_stopping_counter+=1\n",
    "\n",
    "        scheduler.step()           \n",
    "    \n",
    "    model.load_state_dict(best_model_wts)       \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and evaluation\n",
    "\n",
    "def test_result(test_model, test_dataloader):\n",
    "\n",
    "  test_model.eval()\n",
    "  all_test_output_probas=[]\n",
    "\n",
    "  all_test_labels=[]\n",
    "  sig=torch.nn.Sigmoid()  # 이진분류 처리를 위해 BCEWithLogitLoss 함수와 함께 Sigmoid 사용\n",
    "\n",
    "  for inputs,labels in test_dataloader['test']:\n",
    "    inputs = inputs.unsqueeze(0)\n",
    "    inputs = inputs.type(torch.FloatTensor)\n",
    "    inputs = inputs.to(config['device'])\n",
    "    \n",
    "    labels = labels.to(config['device'])\n",
    "\n",
    "    with torch.set_grad_enabled(False):\n",
    "      outputs = test_model(inputs)\n",
    "      outputs=outputs.squeeze()\n",
    "      outputs=sig(outputs)         \n",
    "      outputs=outputs.cpu().detach().numpy()\n",
    "      labels=labels.cpu().detach().numpy()\n",
    "\n",
    "      for i in range(outputs.shape[0]):\n",
    "         all_test_labels.append(labels[i])\n",
    "         all_test_output_probas.append(outputs[i])\n",
    "         \n",
    "  performance_value=roc_auc_score(all_test_labels,all_test_output_probas)\n",
    "  print(\"AUROC score: \",performance_value)\n",
    "\n",
    "  return all_test_labels, all_test_output_probas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (22792, 14)\n",
      "Val: (3256, 14)\n",
      "Test: (6512, 14)\n"
     ]
    }
   ],
   "source": [
    "# Data loading and data split\n",
    "\n",
    "# 오리지널 데이터 사용\n",
    "x_data,y_data=read_data(dataset_name=config['DATASET_NAME'])\n",
    "\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_data,y_data,test_size=0.2,random_state=config['SEED'],stratify=y_data,shuffle=True)\n",
    "val_size=int(len(y_data)*0.1)\n",
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=val_size,random_state=config['SEED'],stratify=y_train, shuffle=True)\n",
    "\n",
    "print(\"Train:\",x_train.shape)\n",
    "print(\"Val:\",x_val.shape)\n",
    "print(\"Test:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Lodader wrapper\n",
    "\n",
    "class TabularDataLoader(Dataset):\n",
    "\n",
    "    def __init__(self,length,data_type):\n",
    "        self.length=length\n",
    "        self.data_type=data_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_type=='train':\n",
    "            return x_train[idx],y_train[idx]\n",
    "        if self.data_type=='val':\n",
    "            return x_val[idx],y_val[idx]\n",
    "        if self.data_type=='test':\n",
    "           return x_test[idx],y_test[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MambaTab(\n",
       "  (linear_layer): Linear(in_features=14, out_features=32, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "  (mamba): Mamba(\n",
       "    (in_proj): Linear(in_features=32, out_features=128, bias=False)\n",
       "    (conv1d): Conv1d(64, 64, kernel_size=(4,), stride=(1,), padding=(3,), groups=64)\n",
       "    (act): SiLU()\n",
       "    (x_proj): Linear(in_features=64, out_features=66, bias=False)\n",
       "    (dt_proj): Linear(in_features=2, out_features=64, bias=True)\n",
       "    (out_proj): Linear(in_features=64, out_features=32, bias=False)\n",
       "  )\n",
       "  (output_layer): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preparing Dataloaders\n",
    "train_set = TabularDataLoader(length=x_train.shape[0],data_type='train')\n",
    "val_set = TabularDataLoader(length=x_val.shape[0],data_type='val')\n",
    "test_set = TabularDataLoader(length=x_test.shape[0],data_type='test')\n",
    "\n",
    "dataloader = {\n",
    "      'train': DataLoader(train_set, batch_size=config['BATCH'], shuffle=True, num_workers=0),\n",
    "      'val': DataLoader(val_set, batch_size=config['BATCH'], shuffle=False, num_workers=0),\n",
    "      'test': DataLoader(test_set, batch_size=config['BATCH'], shuffle=False, num_workers=0)\n",
    "   }\n",
    "\n",
    "# Get the model: \"n_class=1 is to use a single output logit strategy,  where n_class does not refer to the number of classes and is sufficient for binary classification\"\n",
    "model=MambaTab(input_features=x_train.shape[1], n_class=1)\n",
    "model=model.to(config['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 57/100 [00:35<00:27,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC score:  0.8981984063346211\n",
      "----------------Complete of original data----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Train-validate the model\n",
    "model=train_model(model,config, dataloader)\n",
    "\n",
    "# Get test set performance\n",
    "test_labels, test_probas = test_result(model, dataloader)\n",
    "print(\"----------------Complete of original data----------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_cleansing: (34189, 2)\n",
      "Val_cleansing: (4884, 2)\n",
      "Test_cleansing: (9769, 2)\n"
     ]
    }
   ],
   "source": [
    "# 같은 조 공통 데이터 (성준님 클렌징 데이터) 사용\n",
    "\n",
    "x_data_c, y_data_c, _ = load_and_preprocess_data()\n",
    "\n",
    "x_train_c, x_test_c, y_train_c, y_test_c = train_test_split(x_data_c, y_data_c, test_size = 0.2, random_state = config['SEED'], stratify = y_data_c, shuffle=True)\n",
    "val_size_c = int(len(y_data_c)*0.1)\n",
    "x_train_c, x_val_c, y_train_c, y_val_c = train_test_split(x_train_c, y_train_c, test_size = val_size_c, random_state = config['SEED'], stratify = y_train_c, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Lodader wrapper\n",
    "\n",
    "class TabularDataLoader_c(Dataset):\n",
    "\n",
    "    def __init__(self,length,data_type):\n",
    "        self.length=length\n",
    "        self.data_type=data_type\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.data_type=='train':\n",
    "            return x_train_c[idx],y_train_c[idx]\n",
    "        if self.data_type=='val':\n",
    "            return x_val_c[idx],y_val_c[idx]\n",
    "        if self.data_type=='test':\n",
    "           return x_test_c[idx],y_test_c[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train_cleansing: (34189, 2)\n",
      "Val_cleansing: (4884, 2)\n",
      "Test_cleansing: (9769, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:42<01:03,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUROC score:  0.7097034088003703\n",
      "----------------Complete of cleansing data----------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_set_c = TabularDataLoader_c(length = x_train_c.shape[0], data_type='train')\n",
    "val_set_c = TabularDataLoader_c(length = x_val_c.shape[0], data_type='val')\n",
    "test_set_c = TabularDataLoader_c(length = x_test_c.shape[0], data_type='test')\n",
    "\n",
    "print(\"Train_cleansing:\", x_train_c.shape)\n",
    "print(\"Val_cleansing:\", x_val_c.shape)\n",
    "print(\"Test_cleansing:\", x_test_c.shape)\n",
    "\n",
    "dataloader_c = {\n",
    "      'train': DataLoader(train_set_c, batch_size=config['BATCH'], shuffle=True, num_workers=0),\n",
    "      'val': DataLoader(val_set_c, batch_size=config['BATCH'], shuffle=False, num_workers=0),\n",
    "      'test': DataLoader(test_set_c, batch_size=config['BATCH'], shuffle=False, num_workers=0)\n",
    "   }\n",
    "\n",
    "model_c = MambaTab(input_features = x_train_c.shape[1], n_class=1)\n",
    "model_c = model_c.to(config['device'])\n",
    "\n",
    "model_c = train_model(model_c, config, dataloader_c)\n",
    "\n",
    "# Get test set performance\n",
    "test_labels_c, test_probas_c = test_result(model_c, dataloader_c)\n",
    "print(\"----------------Complete of cleansing data----------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambatab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
