{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 기계학습특론 Final Project 10조 - 개별 구현\n",
    "## 2024712428 인공지능융합학과 노희섭\n",
    "#### MambaTab (supervised learning) for classification of adult dataset with fairness evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch==2.1.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision==0.16.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (0.16.1)\n",
      "Requirement already satisfied: filelock in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch==2.1.1) (2.1.0)\n",
      "Requirement already satisfied: numpy in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torchvision==0.16.1) (1.26.4)\n",
      "Requirement already satisfied: requests in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torchvision==0.16.1) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torchvision==0.16.1) (11.0.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.1) (12.6.85)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from jinja2->torch==2.1.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->torchvision==0.16.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->torchvision==0.16.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->torchvision==0.16.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->torchvision==0.16.1) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from sympy->torch==2.1.1) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: causal-conv1d==1.1.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: torch in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from causal-conv1d==1.1.1) (2.1.1)\n",
      "Requirement already satisfied: packaging in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from causal-conv1d==1.1.1) (24.2)\n",
      "Requirement already satisfied: buildtools in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from causal-conv1d==1.1.1) (1.0.6)\n",
      "Requirement already satisfied: ninja in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from causal-conv1d==1.1.1) (1.11.1.2)\n",
      "Requirement already satisfied: sqlalchemy in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from buildtools->causal-conv1d==1.1.1) (2.0.36)\n",
      "Collecting argparse (from buildtools->causal-conv1d==1.1.1)\n",
      "  Using cached argparse-1.4.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: twisted in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from buildtools->causal-conv1d==1.1.1) (24.10.0)\n",
      "Requirement already satisfied: simplejson in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from buildtools->causal-conv1d==1.1.1) (3.19.3)\n",
      "Requirement already satisfied: furl in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from buildtools->causal-conv1d==1.1.1) (2.1.3)\n",
      "Requirement already satisfied: requests in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from buildtools->causal-conv1d==1.1.1) (2.32.3)\n",
      "Requirement already satisfied: docopt in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from buildtools->causal-conv1d==1.1.1) (0.6.2)\n",
      "Requirement already satisfied: python-dateutil in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from buildtools->causal-conv1d==1.1.1) (2.9.0.post0)\n",
      "Requirement already satisfied: jinja2 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from buildtools->causal-conv1d==1.1.1) (3.1.4)\n",
      "Requirement already satisfied: redo in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from buildtools->causal-conv1d==1.1.1) (3.0.0)\n",
      "Requirement already satisfied: filelock in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (3.4.2)\n",
      "Requirement already satisfied: fsspec in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->causal-conv1d==1.1.1) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->causal-conv1d==1.1.1) (12.6.85)\n",
      "Requirement already satisfied: six>=1.8.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from furl->buildtools->causal-conv1d==1.1.1) (1.16.0)\n",
      "Requirement already satisfied: orderedmultidict>=1.0.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from furl->buildtools->causal-conv1d==1.1.1) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from jinja2->buildtools->causal-conv1d==1.1.1) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->buildtools->causal-conv1d==1.1.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->buildtools->causal-conv1d==1.1.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->buildtools->causal-conv1d==1.1.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->buildtools->causal-conv1d==1.1.1) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from sqlalchemy->buildtools->causal-conv1d==1.1.1) (3.1.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from sympy->torch->causal-conv1d==1.1.1) (1.3.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from twisted->buildtools->causal-conv1d==1.1.1) (24.2.0)\n",
      "Requirement already satisfied: automat>=24.8.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from twisted->buildtools->causal-conv1d==1.1.1) (24.8.1)\n",
      "Requirement already satisfied: constantly>=15.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from twisted->buildtools->causal-conv1d==1.1.1) (23.10.4)\n",
      "Requirement already satisfied: hyperlink>=17.1.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from twisted->buildtools->causal-conv1d==1.1.1) (21.0.0)\n",
      "Requirement already satisfied: incremental>=24.7.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from twisted->buildtools->causal-conv1d==1.1.1) (24.7.2)\n",
      "Requirement already satisfied: zope-interface>=5 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from twisted->buildtools->causal-conv1d==1.1.1) (7.1.1)\n",
      "Requirement already satisfied: setuptools>=61.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from incremental>=24.7.0->twisted->buildtools->causal-conv1d==1.1.1) (75.1.0)\n",
      "Requirement already satisfied: tomli in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from incremental>=24.7.0->twisted->buildtools->causal-conv1d==1.1.1) (2.1.0)\n",
      "Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
      "Installing collected packages: argparse\n",
      "Successfully installed argparse-1.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mamba-ssm in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: torch in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from mamba-ssm) (2.1.1)\n",
      "Requirement already satisfied: packaging in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from mamba-ssm) (24.2)\n",
      "Requirement already satisfied: ninja in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from mamba-ssm) (1.11.1.2)\n",
      "Requirement already satisfied: einops in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from mamba-ssm) (0.8.0)\n",
      "Requirement already satisfied: triton in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from mamba-ssm) (2.1.0)\n",
      "Requirement already satisfied: transformers in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from mamba-ssm) (4.46.3)\n",
      "Requirement already satisfied: filelock in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (4.11.0)\n",
      "Requirement already satisfied: sympy in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from torch->mamba-ssm) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->mamba-ssm) (12.6.85)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from transformers->mamba-ssm) (1.26.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from transformers->mamba-ssm) (5.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from transformers->mamba-ssm) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from transformers->mamba-ssm) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from transformers->mamba-ssm) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from transformers->mamba-ssm) (4.67.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from jinja2->torch->mamba-ssm) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from requests->transformers->mamba-ssm) (2024.8.30)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/jerome75/miniconda3/envs/mambatab_env/lib/python3.10/site-packages (from sympy->torch->mamba-ssm) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# I used python kernel 3.10.15 + nvidia RTX 3090 + cuda 12.1 in local machine\n",
    "\n",
    "# install ralated modules\n",
    "%pip install torch==2.1.1 torchvision==0.16.1   # compatible version of pytorch and torchvision for mamba-ssm \n",
    "%pip install causal-conv1d==1.1.1   # causal dpthwise conv 1d  module in CUDA with pytorch\n",
    "%pip install mamba-ssm  # Mamba block module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder, StandardScaler,MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import tqdm\n",
    "\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_string_dtype\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import copy\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch:  2.1 ; cuda:  cu121\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check environments\n",
    "\n",
    "TORCH_VERSION = \".\".join(torch.__version__.split(\".\")[:2])\n",
    "CUDA_VERSION = torch.__version__.split(\"+\")[-1]\n",
    "print(\"torch: \", TORCH_VERSION, \"; cuda: \", CUDA_VERSION)\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: `./datasets' 디렉터리를 만들 수 없습니다: 파일이 있습니다\n",
      "mkdir: `./datasets/adult' 디렉터리를 만들 수 없습니다: 파일이 있습니다\n",
      "‘adult.zip’ 파일이 이미 있습니다. 가져오지 않음.\n",
      "\n",
      "Archive:  ./adult.zip\n",
      "  inflating: ./datasets/adult/Index  \n",
      "  inflating: ./datasets/adult/adult.data  \n",
      "  inflating: ./datasets/adult/adult.names  \n",
      "  inflating: ./datasets/adult/adult.test  \n",
      "  inflating: ./datasets/adult/old.adult.names  \n"
     ]
    }
   ],
   "source": [
    "# Get original data from UCI\n",
    "\n",
    "!mkdir ./datasets\n",
    "!mkdir ./datasets/adult\n",
    "!wget -nc https://archive.ics.uci.edu/static/public/2/adult.zip\n",
    "!unzip -o ./adult.zip -d ./datasets/adult\n",
    "!cp -rf ./datasets/adult/adult.data ./datasets/adult/data_processed.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Configuration for MambaTab\n",
    "config={\n",
    "    'DATASET_NAME':'adult',\n",
    "    'SEED':15, # random seed 지정\n",
    "    'BATCH':100,\n",
    "    'LR':0.0001,\n",
    "    'EPOCH':1000,\n",
    "    'MAMBA_SSM_DIM':32,  # MAMBA model의 dimension 설정 (d_model: Selective Structured State Machine에 담을 최대 Dimension)\n",
    "    'device':'cuda'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data load and preparing\n",
    "\n",
    "def read_data(dataset_name):\n",
    "    data=pd.read_csv('./datasets/'+dataset_name+'/data_processed'+'.csv')\n",
    "    \n",
    "    # fill null values\n",
    "    for col in data.columns: \n",
    "        #data[col].fillna(data[col].mode()[0], inplace=True)\n",
    "        data[col] = data[col].fillna(data[col].mode()[0])\n",
    "\n",
    "    # categorical encoder: 문자열인 경우 소문자로 통일하고, 숫자로 인코딩 처리\n",
    "    for c in data.columns:\n",
    "        if is_string_dtype(data[c]):\n",
    "            data[c]=data[c].str.lower()\n",
    "            enc=OrdinalEncoder()\n",
    "            cur_data=np.array(data[c])\n",
    "            cur_data=np.reshape(cur_data,(cur_data.shape[0],1))\n",
    "            data[c] = enc.fit_transform(cur_data)\n",
    "\n",
    "    # 마지막 column을 lable로 추출\n",
    "    y_data=data[data.columns[-1]]\n",
    "\n",
    "    # 9번째 컬럼(gender)을 공정성 지표 계산을 위해 별도로 추출\n",
    "    sensitive_data=data[data.columns[9]]\n",
    "\n",
    "    # label 컬럼 제거\n",
    "    x_data = data.drop(labels = [data.columns[-1]],axis = 1)\n",
    "    \n",
    "    # 나머지 컬럼 스케일링 처리\n",
    "    x_data=MinMaxScaler().fit_transform(x_data)\n",
    "    \n",
    "    x_data, y_data, sensitive_data = np.array(x_data),np.array(y_data), np.array(sensitive_data)\n",
    "    \n",
    "    return x_data, y_data, sensitive_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           Male\n",
       "1           Male\n",
       "2           Male\n",
       "3         Female\n",
       "4         Female\n",
       "          ...   \n",
       "32555     Female\n",
       "32556       Male\n",
       "32557     Female\n",
       "32558       Male\n",
       "32559     Female\n",
       "Name:  Male, Length: 32560, dtype: object"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check sensitive column (gendar)\n",
    "\n",
    "d = pd.read_csv('./datasets/adult/data_processed.csv')\n",
    "d[d.columns[9]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MambaTab Class\n",
    "\n",
    "class MambaTab(torch.nn.Module):\n",
    "\n",
    "    def __init__(self,input_features, n_class, intermediate_representation=config['MAMBA_SSM_DIM']):\n",
    "        super(MambaTab, self).__init__()\n",
    "        self.linear_layer=torch.nn.Linear(input_features,intermediate_representation)\n",
    "        self.relu=torch.nn.ReLU()\n",
    "        self.layer_norm=torch.nn.LayerNorm(intermediate_representation)\n",
    "\n",
    "        self.mamba=Mamba(d_model=intermediate_representation, d_state=32, d_conv=4, expand=2) # to fine-tuning\n",
    "        self.output_layer=torch.nn.Linear(intermediate_representation,n_class)\n",
    "    \n",
    "    # 논문의 deault model 참조해서 building\n",
    "    def forward(self, x):\n",
    "         x=self.linear_layer(x)\n",
    "         x=self.layer_norm(x)\n",
    "         x=self.relu(x)\n",
    "         x=self.mamba(x)\n",
    "         x=self.output_layer(x)\n",
    "         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def train_model(model, config, dataloader):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict()) # 최적 가중치 저장\n",
    "    best_loss = 1e10 # 최적 손실값 초기화\n",
    "    early_stopping_counter=0 # earlt stopping 값 초기화\n",
    "\n",
    "    optimizer=torch.optim.Adam(model.parameters(),lr=config['LR'])  # Optimizer setting: Adam\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['EPOCH'], eta_min=0,verbose=False) # learning rate 감소를 위한 scheduler setting\n",
    "    loss_fn=torch.nn.BCEWithLogitsLoss()    # 이진분류 처리를 위해 BCEWithLogitLoss 함수 사용\n",
    "  \n",
    "    # 학습 진행\n",
    "    for epoch in tqdm.tqdm(range(config['EPOCH'])):\n",
    "        if early_stopping_counter>=5:\n",
    "          break\n",
    "        \n",
    "        for phase in ['train', 'val']:      \n",
    "            if phase == 'train':               \n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()  \n",
    "            \n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "          \n",
    "            for btch,feed_dict in enumerate(dataloader[phase]):\n",
    "                inputs=feed_dict[0]\n",
    "                inputs=inputs.unsqueeze(0)\n",
    "                labels=feed_dict[1]\n",
    "                sensitives=feed_dict[2]\n",
    "                \n",
    "                inputs = inputs.type(torch.FloatTensor)\n",
    "                inputs = inputs.to(config['device'])\n",
    "                labels = labels.type(torch.FloatTensor)\n",
    "                labels = labels.to(config['device'])\n",
    "                # sensitives = sensitives.type(torch.FloatTensor)\n",
    "                # sensitives = sensitives.to(config['device'])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'): # 학습 단계에서만 gradient 계산\n",
    "\n",
    "                    outputs = model(inputs)  # 모델 학습\n",
    "                    outputs=outputs.squeeze()  \n",
    "                    loss=loss_fn(outputs,labels) # 손실 계산\n",
    "                    metrics['loss']+=loss.item()\n",
    "                \n",
    "                    if phase == 'train':\n",
    "                        loss.backward() # gradient 계산\n",
    "                        optimizer.step() # graident 업데이트\n",
    "                \n",
    "                epoch_samples += 1 \n",
    "           \n",
    "            epoch_loss = metrics['loss'] / epoch_samples # epoch 총손실 계산\n",
    "\n",
    "            if phase == 'val':\n",
    "           \n",
    "                if epoch_loss<best_loss:\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict()) # 최적 가중치 저장\n",
    "                    best_loss=epoch_loss\n",
    "                    early_stopping_counter=0\n",
    "                else:\n",
    "                    early_stopping_counter+=1\n",
    "            \n",
    "        print(f\"Epoch [{epoch+1}/config['EPOCH'], total loss of epoch: {metrics['loss']}\")\n",
    "\n",
    "        scheduler.step()           \n",
    "    \n",
    "    model.load_state_dict(best_model_wts) # 최적 가중치 불러오기\n",
    "\n",
    "    print (\"training completed\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_result(test_model, test_dataloader):\n",
    "\n",
    "    test_model.eval()\n",
    "    \n",
    "    all_probs=[]\n",
    "    all_labels=[]\n",
    "    all_sensitives = []\n",
    "\n",
    "    sig=torch.nn.Sigmoid()  # 이진분류 처리를 위해 BCEWithLogitLoss 함수와 함께 Sigmoid 사용\n",
    "\n",
    "    for inputs,labels, sensitives in test_dataloader['test']:\n",
    "        \n",
    "        inputs = inputs.unsqueeze(0)\n",
    "        inputs = inputs.type(torch.FloatTensor)\n",
    "        \n",
    "        inputs = inputs.to(config['device'])\n",
    "        labels = labels.to(config['device'])\n",
    "        sensitives = sensitives.to(config['device'])\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = test_model(inputs) # 모델 예측\n",
    "            outputs=outputs.squeeze()\n",
    "            \n",
    "            outputs=sig(outputs)\n",
    "\n",
    "            # Detach 처리         \n",
    "            outputs=outputs.cpu().detach().numpy()\n",
    "            labels=labels.cpu().detach().numpy()\n",
    "            sensitives=sensitives.cpu().detach().numpy()\n",
    "            \n",
    "            # 실제 값, 예측 값, 민감 속성 저장\n",
    "            for i in range(outputs.shape[0]):\n",
    "                all_labels.append(labels[i])\n",
    "                all_probs.append(outputs[i])\n",
    "                all_sensitives.append(sensitives[i])\n",
    "    \n",
    "    print(\"test completed\")\n",
    "\n",
    "    return all_labels, all_probs, all_sensitives \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (22792, 14)\n",
      "Val: (3256, 14)\n",
      "Test: (6512, 14)\n"
     ]
    }
   ],
   "source": [
    "# Data loading and data split\n",
    "\n",
    "x_data, y_data, sensitive_data = read_data(dataset_name=config['DATASET_NAME'])\n",
    "\n",
    "x_train, x_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(x_data, y_data, sensitive_data, test_size=0.2,random_state=config['SEED'],stratify=y_data,shuffle=True)\n",
    "val_size = int(len(y_data)*0.1)\n",
    "x_train, x_val, y_train, y_val, sensitive_train, sensitive_val = train_test_split(x_train, y_train, sensitive_train, test_size=val_size,random_state=config['SEED'],stratify=y_train, shuffle=True)\n",
    "\n",
    "print(\"Train:\",x_train.shape)\n",
    "print(\"Val:\",x_val.shape)\n",
    "print(\"Test:\",x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data from numpy float array to tensor\n",
    "x_train = torch.FloatTensor(x_train)\n",
    "x_val = torch.FloatTensor(x_val)\n",
    "x_test = torch.FloatTensor(x_test)\n",
    "\n",
    "y_train = torch.FloatTensor(y_train)\n",
    "y_val = torch.FloatTensor(y_val)\n",
    "y_test = torch.FloatTensor(y_test)\n",
    "\n",
    "sensitive_train = torch.FloatTensor(sensitive_train)\n",
    "sensitive_val = torch.FloatTensor(sensitive_val)\n",
    "sensitive_test = torch.FloatTensor(sensitive_test)\n",
    "\n",
    "# dataset grouping\n",
    "train_set = TensorDataset(x_train, y_train, sensitive_train)\n",
    "val_set = TensorDataset(x_val, y_val, sensitive_val)\n",
    "test_set = TensorDataset(x_test, y_test, sensitive_test)\n",
    "\n",
    "# build data lodaer\n",
    "dataloader = {\n",
    "      'train': DataLoader(train_set, batch_size=config['BATCH'], shuffle=True, num_workers=4),\n",
    "      'val': DataLoader(val_set, batch_size=config['BATCH'], shuffle=False, num_workers=4),\n",
    "      'test': DataLoader(test_set, batch_size=config['BATCH'], shuffle=False, num_workers=4)\n",
    "   }\n",
    "\n",
    "# Get the model: \"n_class=1 is to use a single output logit strategy,  where n_class does not refer to the number of classes and is sufficient for binary classification\"\n",
    "model=MambaTab(input_features=x_train.shape[1], n_class=1)\n",
    "model=model.to(config['device'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch inputs:  tensor([[0.0000, 0.5000, 0.0696,  ..., 0.0000, 0.1939, 0.9512],\n",
      "        [0.5205, 0.5000, 0.1006,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1096, 0.5000, 0.0752,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.2192, 0.5000, 0.1678,  ..., 0.0000, 0.4082, 0.9512],\n",
      "        [0.2877, 0.5000, 0.1005,  ..., 0.4708, 0.4388, 0.9512],\n",
      "        [0.1507, 0.5000, 0.0274,  ..., 0.0000, 0.5000, 0.9512]])\n",
      "Batch labels:  tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.2055, 0.5000, 0.0099,  ..., 0.0000, 0.8469, 0.9512],\n",
      "        [0.3014, 0.5000, 0.1296,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.1507, 0.5000, 0.0763,  ..., 0.0000, 0.7041, 0.9512],\n",
      "        ...,\n",
      "        [0.3836, 0.1250, 0.2095,  ..., 0.0000, 0.8061, 0.9512],\n",
      "        [0.2877, 0.5000, 0.0186,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0411, 0.0000, 0.1523,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 1., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 0., 1.])\n",
      "Batch inputs:  tensor([[0.3425, 0.5000, 0.3511,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0137, 0.0000, 0.0782,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.7397, 0.6250, 0.0369,  ..., 0.5491, 0.6020, 0.9512],\n",
      "        ...,\n",
      "        [0.2603, 0.5000, 0.0896,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2055, 0.5000, 0.1229,  ..., 0.0000, 0.3980, 0.7317],\n",
      "        [0.3562, 0.5000, 0.1949,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 0., 1., 1.])\n",
      "Batch inputs:  tensor([[0.1233, 0.5000, 0.1650,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0548, 0.5000, 0.1679,  ..., 0.0000, 0.3571, 0.9512],\n",
      "        [0.5342, 0.5000, 0.0543,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.1507, 0.2500, 0.2646,  ..., 0.0000, 0.1939, 0.9512],\n",
      "        [0.1370, 0.5000, 0.1131,  ..., 0.0000, 0.8061, 0.9512],\n",
      "        [0.1644, 0.5000, 0.2767,  ..., 0.0000, 0.3980, 0.5854]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.2740, 0.2500, 0.0631,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0411, 0.5000, 0.1539,  ..., 0.0000, 0.3980, 0.1951],\n",
      "        [0.4658, 0.8750, 0.1527,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.1233, 0.5000, 0.1615,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1096, 0.5000, 0.2596,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4795, 0.8750, 0.2218,  ..., 0.0000, 0.7041, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 0., 1., 0., 1.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.3288, 0.5000, 0.1174,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.5890, 0.7500, 0.0565,  ..., 0.0000, 0.4592, 0.9512],\n",
      "        [0.5616, 0.5000, 0.1239,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.3562, 0.5000, 0.1220,  ..., 0.0000, 0.5510, 0.9512],\n",
      "        [0.0685, 0.5000, 0.1346,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.5479, 0.5000, 0.1074,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 1.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 0., 0., 1.])\n",
      "Batch inputs:  tensor([[0.2055, 0.8750, 0.1366,  ..., 0.3962, 0.3776, 0.9512],\n",
      "        [0.1507, 0.5000, 0.2472,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.6301, 0.5000, 0.0994,  ..., 0.0000, 0.0714, 0.9512],\n",
      "        ...,\n",
      "        [0.3562, 0.5000, 0.1233,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2329, 0.5000, 0.4562,  ..., 0.0000, 0.3980, 0.1951],\n",
      "        [0.4110, 0.5000, 0.1111,  ..., 0.5002, 0.5204, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.0000, 0.5000, 0.1927,  ..., 0.0000, 0.1939, 0.9512],\n",
      "        [0.3151, 0.8750, 0.1119,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        [0.8356, 0.0000, 0.0107,  ..., 0.0000, 0.3163, 0.9512],\n",
      "        ...,\n",
      "        [0.0411, 0.5000, 0.0853,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.1507, 0.5000, 0.1749,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2603, 0.5000, 0.0671,  ..., 0.0000, 0.4490, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.1781, 0.5000, 0.2139,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4795, 0.1250, 0.0170,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2055, 0.2500, 0.0048,  ..., 0.0000, 0.5612, 0.2195],\n",
      "        ...,\n",
      "        [0.6712, 0.7500, 0.0627,  ..., 0.0000, 0.0714, 0.9512],\n",
      "        [0.2740, 0.1250, 0.1884,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1507, 0.0000, 0.1252,  ..., 0.0000, 0.3980, 0.6341]])\n",
      "Batch labels:  tensor([0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.1918, 0.5000, 0.2491,  ..., 0.0000, 0.3980, 0.9756],\n",
      "        [0.3014, 0.1250, 0.2854,  ..., 0.3737, 0.3980, 0.9512],\n",
      "        [0.0548, 0.5000, 0.1687,  ..., 0.0000, 0.3571, 0.9512],\n",
      "        ...,\n",
      "        [0.2740, 0.5000, 0.0417,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2603, 0.5000, 0.4045,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.3425, 0.8750, 0.1983,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 1., 1., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.0137, 0.5000, 0.2815,  ..., 0.0000, 0.3980, 0.6341],\n",
      "        [0.5753, 0.1250, 0.1211,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2329, 0.5000, 0.2496,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.2740, 0.5000, 0.1927,  ..., 0.3737, 0.3980, 0.9512],\n",
      "        [0.0000, 0.5000, 0.0641,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0000, 0.5000, 0.0361,  ..., 0.0000, 0.1122, 0.9512]])\n",
      "Batch labels:  tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.3699, 0.1250, 0.1167,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2603, 0.8750, 0.1144,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2740, 0.5000, 0.1270,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.3836, 0.5000, 0.0577,  ..., 0.0000, 0.1531, 0.9512],\n",
      "        [0.3562, 0.6250, 0.0971,  ..., 0.0000, 0.6020, 0.9512],\n",
      "        [0.2466, 0.5000, 0.0673,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 1., 1.])\n",
      "Batch inputs:  tensor([[0.0685, 0.5000, 0.0245,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2192, 0.2500, 0.0146,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3014, 0.5000, 0.1212,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        ...,\n",
      "        [0.1370, 0.5000, 0.0917,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2329, 0.5000, 0.0328,  ..., 0.2020, 0.6020, 0.9512],\n",
      "        [0.1096, 0.5000, 0.2334,  ..., 0.0000, 1.0000, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1.])\n",
      "Batch inputs:  tensor([[0.1781, 0.8750, 0.1272,  ..., 0.4366, 0.3980, 0.9512],\n",
      "        [0.3288, 0.5000, 0.1304,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1781, 0.7500, 0.1097,  ..., 0.0000, 0.6020, 0.9512],\n",
      "        ...,\n",
      "        [0.3973, 0.5000, 0.0759,  ..., 0.0000, 0.4388, 0.9512],\n",
      "        [0.1918, 0.5000, 0.0370,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.1507, 0.5000, 0.0654,  ..., 0.0000, 0.1429, 0.9512]])\n",
      "Batch labels:  tensor([0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 0., 1., 0.])\n",
      "Batch inputs:  tensor([[0.4247, 0.5000, 0.1306,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.2603, 0.7500, 0.2251,  ..., 0.0000, 0.2959, 0.0000],\n",
      "        [0.0000, 0.5000, 0.1302,  ..., 0.0000, 0.1429, 0.9512],\n",
      "        ...,\n",
      "        [0.3151, 0.1250, 0.2270,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2740, 0.5000, 0.1217,  ..., 0.0000, 0.3776, 0.9512],\n",
      "        [0.3562, 0.0000, 0.0709,  ..., 0.0000, 0.4490, 0.9512]])\n",
      "Batch labels:  tensor([1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 1., 0., 1.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 0., 1.])\n",
      "Batch inputs:  tensor([[0.3151, 0.5000, 0.0695,  ..., 0.0000, 0.2347, 0.9512],\n",
      "        [0.2877, 0.5000, 0.0889,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1233, 0.5000, 0.0369,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        ...,\n",
      "        [0.2192, 0.8750, 0.0939,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4110, 0.5000, 0.2539,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1096, 0.5000, 0.1633,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.4110, 0.5000, 0.0398,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3699, 0.7500, 0.0426,  ..., 0.0000, 0.6020, 0.9756],\n",
      "        [0.3562, 0.5000, 0.1772,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        ...,\n",
      "        [0.3699, 0.5000, 0.1000,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.5890, 0.8750, 0.1212,  ..., 0.0000, 0.3673, 0.9512],\n",
      "        [0.2192, 0.5000, 0.1216,  ..., 0.4366, 0.5000, 0.9512]])\n",
      "Batch labels:  tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 0., 0., 0., 1.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 0., 0., 1.])\n",
      "Batch inputs:  tensor([[0.0548, 0.5000, 0.0275,  ..., 0.3453, 0.3980, 0.9512],\n",
      "        [0.0548, 0.8750, 0.0243,  ..., 0.3678, 0.0918, 0.9512],\n",
      "        [0.0411, 0.5000, 0.1016,  ..., 0.0000, 0.4286, 0.9512],\n",
      "        ...,\n",
      "        [0.3836, 0.5000, 0.1090,  ..., 0.0000, 0.5204, 0.9512],\n",
      "        [0.0685, 0.5000, 0.1079,  ..., 0.0000, 0.4796, 0.0000],\n",
      "        [0.3014, 0.5000, 0.1353,  ..., 0.0000, 0.5000, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 0., 1.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 1., 0., 1.])\n",
      "Batch inputs:  tensor([[0.2192, 0.5000, 0.3847,  ..., 0.0000, 0.3980, 0.0000],\n",
      "        [0.1644, 0.5000, 0.0925,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.0411, 0.5000, 0.1611,  ..., 0.0000, 0.1531, 0.9512],\n",
      "        ...,\n",
      "        [0.3836, 0.5000, 0.0974,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.6849, 0.0000, 0.0772,  ..., 0.0000, 0.0714, 0.9512],\n",
      "        [0.4658, 0.5000, 0.0772,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.4384, 0.5000, 0.0078,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1918, 0.5000, 0.3376,  ..., 0.0000, 0.4490, 0.6341],\n",
      "        [0.1644, 0.5000, 0.0921,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.3151, 0.5000, 0.0728,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2877, 0.7500, 0.0699,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4658, 0.5000, 0.0841,  ..., 0.0000, 0.3776, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 0., 1., 0.])\n",
      "Batch inputs:  tensor([[0.5342, 0.5000, 0.1921,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.1644, 0.5000, 0.1761,  ..., 0.0000, 0.3980, 0.6341],\n",
      "        [0.1644, 0.7500, 0.2564,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        ...,\n",
      "        [0.0548, 0.5000, 0.0925,  ..., 0.0000, 0.2449, 0.9512],\n",
      "        [0.0822, 0.5000, 0.1196,  ..., 0.0000, 0.2449, 0.9512],\n",
      "        [0.0274, 0.5000, 0.0939,  ..., 0.0000, 0.1735, 0.7317]])\n",
      "Batch labels:  tensor([1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 1., 0.])\n",
      "Batch inputs:  tensor([[0.0822, 0.5000, 0.2010,  ..., 0.0000, 0.1939, 0.9512],\n",
      "        [0.1781, 0.5000, 0.2807,  ..., 0.0000, 0.3980, 0.6341],\n",
      "        [0.0959, 0.5000, 0.1652,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.0274, 0.5000, 0.1266,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3014, 0.6250, 0.1054,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.0137, 0.5000, 0.2117,  ..., 0.0000, 0.1939, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.3836, 0.2500, 0.1085,  ..., 0.0000, 0.6020, 0.9512],\n",
      "        [0.3288, 0.5000, 0.1762,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4384, 0.5000, 0.1169,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.3973, 0.5000, 0.1017,  ..., 0.0000, 0.4286, 0.9512],\n",
      "        [0.2603, 0.5000, 0.1704,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1918, 0.5000, 0.0678,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 0., 0., 1.])\n",
      "Batch inputs:  tensor([[0.3014, 0.5000, 0.0944,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        [0.3562, 0.1250, 0.1353,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2466, 0.5000, 0.1653,  ..., 0.0000, 0.2653, 0.9512],\n",
      "        ...,\n",
      "        [0.3425, 0.5000, 0.0149,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.2466, 0.8750, 0.1088,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.5205, 0.5000, 0.1205,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 1.])\n",
      "Batch sensitive features:  tensor([0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 1., 0., 1.])\n",
      "Batch inputs:  tensor([[0.1370, 0.5000, 0.1594,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3288, 0.5000, 0.1189,  ..., 0.0000, 0.4796, 0.9512],\n",
      "        [0.3973, 0.5000, 0.0669,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.3562, 0.5000, 0.2296,  ..., 0.0000, 0.3980, 0.6341],\n",
      "        [0.2466, 0.5000, 0.0981,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1507, 0.2500, 0.0492,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
      "Batch inputs:  tensor([[0.0959, 0.5000, 0.0293,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1370, 0.5000, 0.0921,  ..., 0.0000, 0.0000, 0.9512],\n",
      "        [0.4110, 0.1250, 0.1442,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.0137, 0.5000, 0.0587,  ..., 0.0000, 0.1531, 0.9512],\n",
      "        [0.4110, 0.5000, 0.0917,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2740, 0.5000, 0.1442,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 0., 1., 1.])\n",
      "Batch inputs:  tensor([[0.3425, 0.2500, 0.1125,  ..., 0.0000, 0.5000, 0.0000],\n",
      "        [0.6712, 0.2500, 0.1690,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.6164, 0.7500, 0.2092,  ..., 0.0000, 0.4796, 0.9512],\n",
      "        ...,\n",
      "        [0.0959, 0.0000, 0.2473,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4932, 0.5000, 0.0575,  ..., 0.0000, 0.4592, 0.9512],\n",
      "        [0.4110, 0.7500, 0.0645,  ..., 0.0000, 0.3469, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.2055, 0.5000, 0.1511,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.6027, 0.5000, 0.0095,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1370, 0.5000, 0.4019,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.2192, 0.5000, 0.1307,  ..., 0.0000, 0.4388, 0.9512],\n",
      "        [0.3973, 0.5000, 0.0739,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.4384, 0.6250, 0.0662,  ..., 0.0000, 0.3163, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.2877, 0.7500, 0.2348,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4658, 0.1250, 0.1395,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2603, 0.5000, 0.2590,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        ...,\n",
      "        [0.3562, 0.5000, 0.1100,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0548, 0.0000, 0.1505,  ..., 0.0000, 0.2347, 0.9512],\n",
      "        [0.6164, 0.5000, 0.0391,  ..., 0.0000, 0.5000, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 0., 0., 1.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1.])\n",
      "Batch inputs:  tensor([[0.3425, 0.5000, 0.1416,  ..., 0.0000, 0.3980, 0.7561],\n",
      "        [0.2740, 0.5000, 0.1902,  ..., 0.0000, 0.5000, 0.0000],\n",
      "        [0.6164, 0.2500, 0.0945,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.5205, 0.5000, 0.1269,  ..., 0.0000, 0.4184, 0.9512],\n",
      "        [0.5342, 0.5000, 0.1103,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1781, 0.5000, 0.0976,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 1., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.6301, 0.5000, 0.0596,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1644, 0.5000, 0.0654,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4521, 0.2500, 0.1415,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        ...,\n",
      "        [0.5753, 0.5000, 0.1420,  ..., 0.0000, 0.3980, 0.7317],\n",
      "        [0.2877, 0.7500, 0.1540,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.1096, 0.5000, 0.1219,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
      "Batch inputs:  tensor([[0.0685, 0.5000, 0.1518,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.2740, 0.5000, 0.2117,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2192, 0.6250, 0.0861,  ..., 0.0000, 0.8469, 0.8780],\n",
      "        ...,\n",
      "        [0.3562, 0.5000, 0.1535,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3562, 0.5000, 0.1349,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0137, 0.5000, 0.1725,  ..., 0.0000, 0.3469, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 0., 0.])\n",
      "Batch inputs:  tensor([[0.2877, 0.5000, 0.2093,  ..., 0.0000, 0.3980, 0.0000],\n",
      "        [0.0548, 0.8750, 0.1948,  ..., 0.0000, 0.1429, 0.9512],\n",
      "        [0.0959, 0.5000, 0.2292,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.0685, 0.5000, 0.2108,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0822, 0.7500, 0.1836,  ..., 0.0000, 0.6020, 0.9512],\n",
      "        [0.4795, 0.5000, 0.0788,  ..., 0.0000, 0.1327, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.1370, 0.1250, 0.1796,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3014, 0.5000, 0.0712,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1096, 0.5000, 0.1776,  ..., 0.0000, 0.3673, 0.9512],\n",
      "        ...,\n",
      "        [0.3562, 0.1250, 0.1866,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        [0.3151, 0.5000, 0.0291,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.5753, 0.5000, 0.1061,  ..., 0.0000, 0.5000, 0.9512]])\n",
      "Batch labels:  tensor([1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 1., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
      "Batch inputs:  tensor([[0.0274, 0.8750, 0.1039,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1918, 0.7500, 0.0141,  ..., 0.0000, 0.4490, 0.2195],\n",
      "        [0.6712, 0.5000, 0.1392,  ..., 0.0000, 0.0918, 0.9512],\n",
      "        ...,\n",
      "        [0.1233, 0.5000, 0.2347,  ..., 0.0000, 0.1429, 0.9512],\n",
      "        [0.5068, 0.7500, 0.0981,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.0411, 0.5000, 0.0540,  ..., 0.0000, 0.0714, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 1., 0.])\n",
      "Batch inputs:  tensor([[0.2877, 0.8750, 0.0652,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3288, 0.7500, 0.1298,  ..., 0.0000, 0.1429, 0.9512],\n",
      "        [0.2192, 0.5000, 0.0998,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.0411, 0.5000, 0.1787,  ..., 0.0000, 0.2449, 0.9512],\n",
      "        [0.0274, 0.2500, 0.1345,  ..., 0.3951, 0.2959, 0.9512],\n",
      "        [0.2603, 0.5000, 0.0731,  ..., 0.0000, 0.3469, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 0., 0., 0.])\n",
      "Batch inputs:  tensor([[0.5068, 0.5000, 0.1852,  ..., 0.0000, 0.4286, 0.5366],\n",
      "        [0.1096, 0.5000, 0.0743,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4795, 0.5000, 0.0372,  ..., 0.0000, 0.4388, 0.9512],\n",
      "        ...,\n",
      "        [0.3836, 0.8750, 0.0801,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0959, 0.5000, 0.0390,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0959, 0.5000, 0.0617,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 1., 0.])\n",
      "Batch inputs:  tensor([[0.2466, 0.6250, 0.1186,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.0137, 0.5000, 0.1206,  ..., 0.0000, 0.2347, 0.9512],\n",
      "        [0.0548, 0.5000, 0.0776,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.2329, 0.5000, 0.1763,  ..., 0.3997, 0.4490, 0.9512],\n",
      "        [0.2877, 0.0000, 0.0559,  ..., 0.0000, 0.5000, 0.0000],\n",
      "        [0.2192, 0.2500, 0.1907,  ..., 0.0000, 0.3980, 0.0000]])\n",
      "Batch labels:  tensor([1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 1., 0., 0., 1.])\n",
      "Batch inputs:  tensor([[0.0411, 0.5000, 0.1616,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4658, 0.5000, 0.2039,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3699, 0.6250, 0.0957,  ..., 0.0000, 0.5204, 0.9512],\n",
      "        ...,\n",
      "        [0.2877, 0.5000, 0.1434,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3014, 0.5000, 0.0878,  ..., 0.5611, 0.4490, 0.9512],\n",
      "        [0.3288, 0.5000, 0.2799,  ..., 0.0000, 0.4490, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.3151, 0.5000, 0.1403,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.6027, 0.5000, 0.0997,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0959, 0.5000, 0.0747,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.3014, 0.7500, 0.1623,  ..., 0.0000, 0.3980, 0.1220],\n",
      "        [0.1918, 0.5000, 0.1525,  ..., 0.0000, 0.6020, 0.9512],\n",
      "        [0.1233, 0.5000, 0.0822,  ..., 0.0000, 0.4184, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.4247, 0.2500, 0.0610,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        [0.2877, 0.5000, 0.0998,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3014, 0.8750, 0.1543,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        ...,\n",
      "        [0.4247, 0.5000, 0.0096,  ..., 0.0000, 0.1939, 0.9512],\n",
      "        [0.3425, 0.7500, 0.0457,  ..., 0.0000, 0.5510, 0.9512],\n",
      "        [0.1370, 0.0000, 0.1823,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 0., 1., 0.])\n",
      "Batch inputs:  tensor([[0.0685, 0.5000, 0.1200,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3699, 0.5000, 0.1288,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        [0.0822, 0.5000, 0.1006,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.2877, 0.5000, 0.1512,  ..., 0.0000, 0.2959, 0.6341],\n",
      "        [0.2055, 0.8750, 0.2634,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2329, 0.5000, 0.1263,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.1644, 0.6250, 0.0765,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0548, 0.5000, 0.1040,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2740, 0.5000, 0.1584,  ..., 0.0000, 0.3980, 0.0000],\n",
      "        ...,\n",
      "        [0.2740, 0.5000, 0.1086,  ..., 0.0000, 0.2959, 0.9512],\n",
      "        [0.3014, 0.5000, 0.0252,  ..., 0.3168, 0.3980, 0.9512],\n",
      "        [0.3699, 0.2500, 0.0141,  ..., 0.0000, 0.5510, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 0., 0., 1.])\n",
      "Batch inputs:  tensor([[0.0959, 0.5000, 0.0740,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.0685, 0.5000, 0.1993,  ..., 0.0000, 0.4490, 0.0488],\n",
      "        [0.3562, 0.5000, 0.0959,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        ...,\n",
      "        [0.5068, 0.7500, 0.0317,  ..., 0.3625, 0.4796, 0.9512],\n",
      "        [0.1096, 0.5000, 0.1089,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2055, 0.5000, 0.1575,  ..., 0.0000, 0.0918, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 0., 0., 1., 0., 1.])\n",
      "Batch inputs:  tensor([[0.5753, 0.2500, 0.0612,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.1096, 0.5000, 0.1199,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3699, 0.5000, 0.0917,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.6027, 0.0000, 0.0607,  ..., 0.0000, 0.2449, 0.9512],\n",
      "        [0.3014, 0.5000, 0.1937,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0411, 0.5000, 0.2487,  ..., 0.0000, 0.2449, 0.9512]])\n",
      "Batch labels:  tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 1., 1., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.4247, 0.5000, 0.1102,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3288, 0.5000, 0.1069,  ..., 0.0000, 0.3980, 0.0000],\n",
      "        [0.2740, 0.5000, 0.1199,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.2877, 0.5000, 0.1768,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.3288, 0.2500, 0.3236,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2192, 0.5000, 0.0957,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.3288, 0.5000, 0.1221,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3836, 0.2500, 0.1523,  ..., 0.0000, 0.5510, 0.9512],\n",
      "        [0.2877, 0.7500, 0.0281,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        ...,\n",
      "        [0.5479, 0.5000, 0.0836,  ..., 0.0000, 0.3980, 0.7317],\n",
      "        [0.2466, 0.5000, 0.0127,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.4795, 0.2500, 0.1192,  ..., 0.0000, 0.3571, 0.9512]])\n",
      "Batch labels:  tensor([0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 1.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.0137, 0.5000, 0.0735,  ..., 0.0000, 0.1429, 0.9512],\n",
      "        [0.3014, 0.2500, 0.0686,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.7808, 0.7500, 0.1269,  ..., 0.0000, 0.0714, 0.2683],\n",
      "        ...,\n",
      "        [0.3973, 0.0000, 0.1818,  ..., 0.0000, 0.3980, 0.6341],\n",
      "        [0.2603, 0.5000, 0.0522,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.0959, 0.5000, 0.1348,  ..., 0.0000, 0.3980, 0.2439]])\n",
      "Batch labels:  tensor([0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.4110, 0.0000, 0.2169,  ..., 0.0000, 0.0714, 0.9512],\n",
      "        [0.0548, 0.0000, 0.2332,  ..., 0.0000, 0.0918, 0.9512],\n",
      "        [0.1370, 0.5000, 0.2161,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.4384, 0.5000, 0.0557,  ..., 0.0000, 0.6020, 0.9512],\n",
      "        [0.1918, 0.5000, 0.3092,  ..., 0.4332, 0.3980, 0.9512],\n",
      "        [0.5342, 0.7500, 0.2196,  ..., 0.4332, 0.5000, 0.0488]])\n",
      "Batch labels:  tensor([1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 1., 1.])\n",
      "Batch sensitive features:  tensor([0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.3151, 0.5000, 0.1064,  ..., 0.0000, 0.3163, 0.9512],\n",
      "        [0.0411, 0.0000, 0.1492,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3151, 0.5000, 0.0442,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.3014, 0.5000, 0.0394,  ..., 0.0000, 0.3980, 0.7317],\n",
      "        [0.1096, 0.5000, 0.2916,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2603, 0.5000, 0.3204,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 0., 1., 1.])\n",
      "Batch inputs:  tensor([[0.0411, 0.5000, 0.0968,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2329, 0.6250, 0.0462,  ..., 0.0000, 0.7245, 0.9512],\n",
      "        [0.0274, 0.5000, 0.1436,  ..., 0.0000, 0.2959, 0.9512],\n",
      "        ...,\n",
      "        [0.4110, 0.7500, 0.0612,  ..., 0.0000, 0.2959, 0.9512],\n",
      "        [0.3836, 0.6250, 0.0136,  ..., 0.0000, 0.5102, 0.9512],\n",
      "        [0.6438, 0.5000, 0.1420,  ..., 0.0000, 0.1122, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.2192, 0.5000, 0.3648,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2055, 0.5000, 0.0760,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2603, 0.5000, 0.0526,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.5068, 0.6250, 0.0917,  ..., 0.0000, 0.6020, 0.9512],\n",
      "        [0.1507, 0.5000, 0.1263,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2877, 0.5000, 0.2610,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 1., 0., 1.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.0685, 0.5000, 0.0485,  ..., 0.0000, 0.2959, 0.9512],\n",
      "        [0.1918, 0.5000, 0.3229,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.6712, 0.2500, 0.0289,  ..., 0.0000, 0.1939, 0.9512],\n",
      "        ...,\n",
      "        [0.6164, 0.5000, 0.0950,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2329, 0.7500, 0.2101,  ..., 0.0000, 0.2449, 0.9512],\n",
      "        [0.1233, 0.7500, 0.0505,  ..., 0.0000, 0.4490, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
      "Batch inputs:  tensor([[0.5616, 0.5000, 0.1165,  ..., 0.0000, 0.2347, 0.9512],\n",
      "        [0.6027, 0.5000, 0.4456,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2740, 0.5000, 0.2548,  ..., 0.0000, 0.3673, 0.9512],\n",
      "        ...,\n",
      "        [0.4932, 0.6250, 0.0788,  ..., 0.0000, 0.7041, 0.9512],\n",
      "        [0.0685, 0.5000, 0.1379,  ..., 0.0000, 0.3673, 0.9512],\n",
      "        [0.1233, 0.7500, 0.1025,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 1., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 0., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.3151, 0.5000, 0.1035,  ..., 0.0000, 0.3163, 0.9512],\n",
      "        [0.2329, 0.5000, 0.0703,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0822, 0.5000, 0.1547,  ..., 0.0000, 0.2449, 0.9512],\n",
      "        ...,\n",
      "        [0.7945, 0.5000, 0.0629,  ..., 0.0000, 0.1939, 0.9512],\n",
      "        [0.1781, 0.5000, 0.1573,  ..., 0.0000, 0.3980, 0.8537],\n",
      "        [0.5753, 0.7500, 0.1555,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 0.])\n",
      "Batch inputs:  tensor([[0.4384, 0.5000, 0.0478,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.2055, 0.5000, 0.0877,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2740, 0.5000, 0.0911,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.6849, 0.5000, 0.2200,  ..., 0.0000, 0.2347, 0.9512],\n",
      "        [0.2466, 0.8750, 0.1087,  ..., 0.0000, 0.4184, 0.9512],\n",
      "        [0.2192, 0.5000, 0.2086,  ..., 0.0000, 0.6020, 0.7073]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.3425, 0.5000, 0.1520,  ..., 0.0000, 0.3980, 0.8049],\n",
      "        [0.4110, 0.5000, 0.1020,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.3151, 0.8750, 0.0942,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.0822, 0.5000, 0.1483,  ..., 0.0000, 0.2143, 0.9512],\n",
      "        [0.2055, 0.6250, 0.2263,  ..., 0.0000, 0.5510, 0.9512],\n",
      "        [0.1644, 0.5000, 0.0727,  ..., 0.0000, 0.0918, 0.0732]])\n",
      "Batch labels:  tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 1., 1.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0.])\n",
      "Batch inputs:  tensor([[0.6849, 0.5000, 0.1524,  ..., 0.0000, 0.0102, 0.9512],\n",
      "        [0.4110, 0.5000, 0.0577,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0822, 0.5000, 0.1274,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        ...,\n",
      "        [0.4658, 0.5000, 0.1279,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0822, 0.5000, 0.1518,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3288, 0.5000, 0.0374,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1.])\n",
      "Batch sensitive features:  tensor([1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.4932, 0.8750, 0.1247,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1918, 0.5000, 0.2453,  ..., 0.0000, 0.4286, 0.9512],\n",
      "        [0.2055, 0.5000, 0.1553,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.3014, 0.5000, 0.1187,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1918, 0.2500, 0.1199,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.0685, 0.5000, 0.2023,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1.,\n",
      "        1., 0., 0., 1., 0., 1., 0., 0., 0., 1.])\n",
      "Batch inputs:  tensor([[0.4658, 0.7500, 0.0815,  ..., 0.0000, 0.4490, 0.9512],\n",
      "        [0.2192, 0.5000, 0.1624,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.3562, 0.7500, 0.0800,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.0411, 0.5000, 0.1516,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        [0.1370, 0.5000, 0.1737,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.5342, 0.5000, 0.0586,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 0., 1., 1., 0., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.6438, 0.2500, 0.0259,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.4384, 0.8750, 0.2324,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.2329, 0.5000, 0.0258,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.1370, 0.5000, 0.0258,  ..., 0.0000, 0.4796, 0.9512],\n",
      "        [0.3562, 0.5000, 0.0324,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1507, 0.5000, 0.0935,  ..., 0.0000, 0.3980, 0.0000]])\n",
      "Batch labels:  tensor([0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 1., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 1., 1., 1.])\n",
      "Batch inputs:  tensor([[0.5753, 0.5000, 0.2009,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3425, 0.5000, 0.0580,  ..., 0.0000, 0.4694, 0.9512],\n",
      "        [0.6849, 0.5000, 0.2080,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        ...,\n",
      "        [0.0411, 0.1250, 0.0292,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1644, 0.5000, 0.1127,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.5342, 0.2500, 0.1735,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1.,\n",
      "        1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 1., 1., 1., 0., 0., 1., 1.])\n",
      "Batch inputs:  tensor([[0.2329, 0.8750, 0.1206,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.3836, 0.5000, 0.0707,  ..., 0.0000, 0.3469, 0.9512],\n",
      "        [0.0685, 0.5000, 0.1202,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        ...,\n",
      "        [0.2329, 0.8750, 0.0303,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.0822, 0.5000, 0.1106,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1233, 0.5000, 0.1819,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 1., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        1., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 1., 0.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 1.,\n",
      "        0., 0., 0., 1., 1., 0., 0., 0., 0., 1.])\n",
      "Batch inputs:  tensor([[0.2466, 0.5000, 0.2147,  ..., 0.0000, 0.3980, 0.6341],\n",
      "        [0.1644, 0.8750, 0.1737,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.6301, 0.5000, 0.0369,  ..., 0.0000, 0.1531, 0.9512],\n",
      "        ...,\n",
      "        [0.1370, 0.5000, 0.1387,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.1233, 0.5000, 0.0630,  ..., 0.0000, 0.1939, 0.9512],\n",
      "        [0.0274, 0.5000, 0.2705,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
      "        0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 0.,\n",
      "        0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 0., 1., 1., 1., 1., 0., 0., 1., 0.])\n",
      "Batch inputs:  tensor([[0.4795, 0.7500, 0.1209,  ..., 0.0000, 0.2449, 0.9512],\n",
      "        [0.0548, 0.5000, 0.2169,  ..., 0.0000, 0.5000, 0.9512],\n",
      "        [0.3973, 0.5000, 0.1138,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        ...,\n",
      "        [0.0959, 0.0000, 0.0901,  ..., 0.0000, 0.3980, 0.9512],\n",
      "        [0.0000, 0.5000, 0.1375,  ..., 0.0000, 0.2449, 0.9512],\n",
      "        [0.0959, 0.5000, 0.2758,  ..., 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n",
      "        0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 0., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1.,\n",
      "        0., 1., 0., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 1., 0.,\n",
      "        1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 0.,\n",
      "        0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 0., 1.])\n",
      "Batch inputs:  tensor([[0.2192, 0.5000, 0.1643, 0.7333, 0.5333, 0.6667, 0.0714, 0.2000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.3980, 0.9512],\n",
      "        [0.3973, 0.2500, 0.1327, 0.6000, 0.8000, 0.3333, 0.7857, 0.0000, 1.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.3980, 0.9512],\n",
      "        [0.1781, 0.5000, 0.1164, 0.7333, 0.5333, 0.3333, 0.8571, 0.0000, 1.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.4490, 0.9512],\n",
      "        [0.4384, 0.5000, 0.0756, 0.0667, 0.4000, 0.3333, 1.0000, 0.0000, 1.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.7551, 0.9512],\n",
      "        [0.1781, 0.2500, 0.2436, 0.8000, 0.8667, 0.6667, 0.7143, 0.2000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.3980, 0.9512],\n",
      "        [0.3836, 0.7500, 0.0110, 0.6000, 0.8000, 0.3333, 0.3571, 0.0000, 1.0000,\n",
      "         1.0000, 0.0000, 0.3409, 0.7041, 0.9512],\n",
      "        [0.4247, 0.8750, 0.1072, 0.8000, 0.8667, 0.0000, 0.7143, 0.2000, 1.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.3980, 0.9512],\n",
      "        [0.2740, 0.5000, 0.1809, 0.7333, 0.5333, 0.3333, 0.5000, 0.0000, 1.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.6020, 0.9512],\n",
      "        [0.0959, 0.5000, 0.1155, 0.7333, 0.5333, 0.3333, 0.5714, 1.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2449, 0.9512],\n",
      "        [0.5616, 0.2500, 0.1532, 0.6000, 0.8000, 0.3333, 0.7143, 0.0000, 1.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.5816, 0.9512],\n",
      "        [0.2055, 0.5000, 0.1083, 0.7333, 0.5333, 0.6667, 0.5000, 0.2000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.3980, 0.9512],\n",
      "        [0.1370, 0.5000, 0.1257, 0.4667, 0.7333, 0.6667, 0.7143, 0.2000, 1.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.3980, 0.9512]])\n",
      "Batch labels:  tensor([0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0.])\n",
      "Batch sensitive features:  tensor([0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "# check dataloader\n",
    "\n",
    "for batch_data in dataloader['test']:\n",
    "\n",
    "    inputs, labels, sensitive = batch_data\n",
    "    print(\"Batch inputs: \", inputs)\n",
    "    print(\"Batch labels: \", labels)\n",
    "    print(\"Batch sensitive features: \", sensitive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 22792, 32]             480\n",
      "         LayerNorm-2            [-1, 22792, 32]              64\n",
      "              ReLU-3            [-1, 22792, 32]               0\n",
      "            Conv1d-4            [-1, 64, 22795]             320\n",
      "              SiLU-5            [-1, 64, 22792]               0\n",
      "            Linear-6                   [-1, 66]           4,224\n",
      "            Linear-7            [-1, 22792, 32]           2,048\n",
      "             Mamba-8            [-1, 22792, 32]               0\n",
      "            Linear-9             [-1, 22792, 1]              33\n",
      "================================================================\n",
      "Total params: 7,169\n",
      "Trainable params: 7,169\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 1.22\n",
      "Forward/backward pass size (MB): 50.26\n",
      "Params size (MB): 0.03\n",
      "Estimated Total Size (MB): 51.50\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# check model shape\n",
    "\n",
    "summary(model, x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1000 [00:01<20:11,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/config['EPOCH'], total loss of epoch: 11.945384055376053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/1000 [00:02<22:14,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/config['EPOCH'], total loss of epoch: 11.842392474412918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 3/1000 [00:04<22:43,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/config['EPOCH'], total loss of epoch: 11.734708935022354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1000 [00:05<22:56,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/config['EPOCH'], total loss of epoch: 11.657463103532791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1000 [00:06<22:58,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/config['EPOCH'], total loss of epoch: 11.616464421153069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1000 [00:08<23:16,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/config['EPOCH'], total loss of epoch: 11.572381675243378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1000 [00:09<23:12,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/config['EPOCH'], total loss of epoch: 11.54067286849022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1000 [00:11<23:02,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/config['EPOCH'], total loss of epoch: 11.559610575437546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1000 [00:12<22:57,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/config['EPOCH'], total loss of epoch: 11.582268476486206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1000 [00:13<22:46,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/config['EPOCH'], total loss of epoch: 11.430698961019516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 11/1000 [00:15<22:58,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/config['EPOCH'], total loss of epoch: 11.440102905035019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1000 [00:16<22:59,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/config['EPOCH'], total loss of epoch: 11.409653097391129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 13/1000 [00:18<23:25,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/config['EPOCH'], total loss of epoch: 11.532442301511765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 14/1000 [00:19<24:04,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/config['EPOCH'], total loss of epoch: 11.455021783709526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 15/1000 [00:21<23:46,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/config['EPOCH'], total loss of epoch: 11.410488575696945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 16/1000 [00:22<23:42,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/config['EPOCH'], total loss of epoch: 11.300887867808342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 17/1000 [00:23<23:31,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/config['EPOCH'], total loss of epoch: 11.321252703666687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 18/1000 [00:25<23:24,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/config['EPOCH'], total loss of epoch: 11.30817560851574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/1000 [00:26<23:21,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/config['EPOCH'], total loss of epoch: 11.2349653840065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/1000 [00:28<23:26,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/config['EPOCH'], total loss of epoch: 11.258623123168945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 21/1000 [00:29<23:25,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/config['EPOCH'], total loss of epoch: 11.243380188941956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 22/1000 [00:31<23:15,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/config['EPOCH'], total loss of epoch: 11.236345559358597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 23/1000 [00:32<23:06,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/config['EPOCH'], total loss of epoch: 11.189532041549683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 24/1000 [00:33<23:04,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/config['EPOCH'], total loss of epoch: 11.173528999090195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 25/1000 [00:35<23:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/config['EPOCH'], total loss of epoch: 11.18307463824749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 26/1000 [00:36<23:03,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/config['EPOCH'], total loss of epoch: 11.191311433911324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 27/1000 [00:38<23:10,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/config['EPOCH'], total loss of epoch: 11.158059999346733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 28/1000 [00:39<23:12,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/config['EPOCH'], total loss of epoch: 11.15107537806034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 29/1000 [00:41<23:12,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/config['EPOCH'], total loss of epoch: 11.300334498286247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 30/1000 [00:42<22:59,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/config['EPOCH'], total loss of epoch: 11.139139354228973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 31/1000 [00:43<22:57,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/config['EPOCH'], total loss of epoch: 11.10478362441063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 32/1000 [00:45<22:50,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/config['EPOCH'], total loss of epoch: 11.11321622133255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 33/1000 [00:46<23:00,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/config['EPOCH'], total loss of epoch: 11.16769702732563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 34/1000 [00:48<22:59,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/config['EPOCH'], total loss of epoch: 11.146885931491852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 35/1000 [00:49<23:03,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/config['EPOCH'], total loss of epoch: 11.09802433848381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 36/1000 [00:51<22:56,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/config['EPOCH'], total loss of epoch: 11.076928064227104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 37/1000 [00:52<22:54,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/config['EPOCH'], total loss of epoch: 11.132921263575554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 38/1000 [00:53<22:49,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/config['EPOCH'], total loss of epoch: 11.08854416012764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 39/1000 [00:55<22:43,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/config['EPOCH'], total loss of epoch: 11.264871552586555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 40/1000 [00:56<22:51,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/config['EPOCH'], total loss of epoch: 11.047472670674324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 41/1000 [00:58<22:56,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/config['EPOCH'], total loss of epoch: 11.067610308527946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 42/1000 [00:59<23:45,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/config['EPOCH'], total loss of epoch: 11.058877035975456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 43/1000 [01:01<23:32,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/config['EPOCH'], total loss of epoch: 11.063371896743774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 44/1000 [01:02<23:18,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/config['EPOCH'], total loss of epoch: 11.0662981569767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 45/1000 [01:04<22:40,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/config['EPOCH'], total loss of epoch: 11.048173800110817\n",
      "training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Train-validate the model\n",
    "model=train_model(model, config, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test completed\n"
     ]
    }
   ],
   "source": [
    "# Test model\n",
    "all_labels, all_probs, all_sensitives = test_result(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fairness_metrics(y_true, y_pred, sensitive_attributes):\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Demographic Parity: P(Y_pred=1 | A=0) = P(Y_pred=1 | A=1)\n",
    "    dp_0 = np.mean(y_pred[sensitive_attributes == 0])\n",
    "    dp_1 = np.mean(y_pred[sensitive_attributes == 1])\n",
    "    \n",
    "    metrics['Demographic Parity'] = abs(dp_0 - dp_1)\n",
    "    \n",
    "    # Equal Opportunity: P(Y_pred=1 | Y_true=1, A=0) = P(Y_pred=1 | Y_true=1, A=1)\n",
    "    eo_0 = np.mean(y_pred[(y_true == 1) & (sensitive_attributes == 0)])\n",
    "    eo_1 = np.mean(y_pred[(y_true == 1) & (sensitive_attributes == 1)])\n",
    "    \n",
    "    metrics['Equal Opportunity'] = abs(eo_0 - eo_1)\n",
    "    \n",
    "    # Equality of Odds: Same true positive and false positive rates for both groups\n",
    "    tp_0 = np.mean(y_pred[(y_true == 1) & (sensitive_attributes == 0)])\n",
    "    fp_0 = np.mean(y_pred[(y_true == 0) & (sensitive_attributes == 0)])\n",
    "    tp_1 = np.mean(y_pred[(y_true == 1) & (sensitive_attributes == 1)])\n",
    "    fp_1 = np.mean(y_pred[(y_true == 0) & (sensitive_attributes == 1)])\n",
    "    \n",
    "    metrics['Equality of Odds'] = abs(tp_0 - tp_1) + abs(fp_0 - fp_1)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AUROC score:  0.9001463715036987\n",
      "\n",
      "공정성 지표\n",
      "\n",
      "  Demographic Parity: 0.19663691520690918\n",
      "  Equal Opportunity: 0.04926776885986328\n",
      "  Equality of Odds: 0.17487171292304993\n"
     ]
    }
   ],
   "source": [
    " # AUROC 계산        \n",
    "auroc_score = roc_auc_score(all_labels, all_probs)\n",
    "print(\"\\nAUROC score: \", auroc_score)\n",
    "  \n",
    "# 공성정 지표 계산\n",
    "fairness_scores = calculate_fairness_metrics(np.array(all_labels), np.array(all_probs), np.array(all_sensitives))\n",
    "\n",
    "print(\"\\n공정성 지표\\n\")\n",
    "for metric, score in fairness_scores.items():\n",
    "    print(f\"  {metric}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#ROC Curve visualization\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_probs)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUROC = {auroc_score:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambatab_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
